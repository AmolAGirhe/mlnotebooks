{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3A2XBqYQg-N"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw4QHJlfsSF4"
      },
      "source": [
        "## Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the mini-hackathon you will be able to:\n",
        "* Perform Data preprocessing\n",
        "* Apply different ML algorithms on the **Titanic** dataset\n",
        "* Perform VotingClassifier\n"
      ],
      "metadata": {
        "id": "EsSB95vPsoxS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh70dVHx0G_B"
      },
      "source": [
        "## Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-GMJTRb0Iyy"
      },
      "source": [
        "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
        "\n",
        "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of many passengers and crew.\n",
        "\n",
        "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
        "\n",
        "[ Data Set Link: Kaggle competition](https://www.kaggle.com/competitions/titanic)\n",
        "\n",
        "<br/>\n",
        "\n",
        "### Data Set Characteristics:\n",
        "\n",
        "**PassengerId:** Id of the Passenger\n",
        "\n",
        "**Survived:** Survived or Not information\n",
        "\n",
        "**Pclass:** Socio-economic status (SES)\n",
        "  * 1st = Upper\n",
        "  * 2nd = Middle\n",
        "  * 3rd = Lower\n",
        "\n",
        "**Name:** Surname, First Names of the Passenger\n",
        "\n",
        "**Sex:** Gender of the Passenger\n",
        "\n",
        "**Age:** Age of the Passenger\n",
        "\n",
        "**SibSp:**\tNo. of siblings/spouse of the passenger aboard the Titanic\n",
        "\n",
        "**Parch:**\tNo. of parents/children of the passenger aboard the Titanic\n",
        "\n",
        "**Ticket:**\tTicket number\n",
        "\n",
        "**Fare:** Passenger fare\n",
        "\n",
        "**Cabin:**\tCabin number\n",
        "\n",
        "**Embarked:** Port of Embarkation\n",
        "  * S = Southampton\n",
        "  * C = Cherbourg\n",
        "  * Q = Queenstown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Build a predictive model that answers the question: “what sort of people were more likely to survive?” using titanic's passenger data (ie name, age, gender, socio-economic class, etc)."
      ],
      "metadata": {
        "id": "KmusUbENKSEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the datasets\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/titanic.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/test_titanic.csv\")\n",
        "    print(\"Data downloaded successfully\")\n",
        "    return\n",
        "\n",
        "setup()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ViFc50xKK-tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "RM8x-pMDLQuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fBLdQaJtd4"
      },
      "source": [
        "## Exercise 1 - Load and Explore the Data (2 Marks)\n",
        "\n",
        "* Understand different features in the training dataset\n",
        "* Understand the data types of each column\n",
        "* Notice the columns of missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6HQH7abkyL"
      },
      "source": [
        "#### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Load the given dataset\n",
        "dataset = pd.read_csv('/content/titanic.csv')"
      ],
      "metadata": {
        "id": "4tUzpuurwCyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ktmr8lh8AS",
        "collapsed": true
      },
      "source": [
        "# Getting information about the dataset\n",
        "dataset.info()\n",
        "\n",
        "#NOTE: In the output check the non-null count information. Age - 714/891; Cabin - 204/891; Embarked - 889/891"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "zAvxH5MUPWDj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 02: Split the data into train and test sets (1 Mark)\n",
        "Note: Apply all your data preprocessing steps in the train set first and keep the test set aside."
      ],
      "metadata": {
        "id": "eQGya6YLOku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Target - Survived\n",
        "y = dataset.Survived\n",
        "\n",
        "#Load Features\n",
        "X = dataset.drop(columns = ['Survived'])\n",
        "\n",
        "# check shape of X & y\n",
        "X.shape, y.shape, type(X), type(y)\n"
      ],
      "metadata": {
        "id": "mps-O7zbPPcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Perform train, test and split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Get columns list\n",
        "X_train_col_list = X_train.columns.tolist()\n",
        "# Convert x_train to Dataframe\n",
        "X_train_df = pd.DataFrame(X_train, columns=X_train_col_list)\n",
        "X_test_col_list = X_test.columns.tolist()\n",
        "X_test_df = pd.DataFrame(X_test, columns=X_test_col_list)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "dUmdpi_ETCuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pMXWDyaspe3"
      },
      "source": [
        "## Exercise 03: Data Cleaning and Processing (15 Marks)\n",
        "### 3.1 Working on the \"Cabin\" column (2 Marks)\n",
        "Find unique entries in the Cabin column. We can label all passengers in two categories having a cabin or not. Check the data type(use: type) of each entry of the Cabin. Convert a string data type into '1' i.e. passengers with cabin and others into '0' i.e. passengers without cabin.  Write a function for the above operation and apply it to the cabin column and create another column with the name \" Has_cabin\" containing only 0 or 1 entries.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to create \"Has_cabin\" column and write 1 if passengers with cabin and 0 if passesngers with no cabin.\n",
        "def ConvertStrToNum(ds, col_name):\n",
        "  ds['Has_cabin'] = ds[col_name].notnull().astype(int)"
      ],
      "metadata": {
        "id": "LEuI7X9Q59wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_df.shape[0])\n",
        "print(X_train_df['Cabin'].info())"
      ],
      "metadata": {
        "id": "agrJ0Plk7G-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call the method to create Has_cabin column. #### Use apply method.\n",
        "for Idx in range(X_train_df.shape[0]):\n",
        "  ConvertStrToNum(X_train_df, 'Cabin')"
      ],
      "metadata": {
        "id": "8TFinMSMLwks",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if changes are done\n",
        "print(X_train_df['Has_cabin'].info())\n",
        "print(X_train_df['Has_cabin'].sum())"
      ],
      "metadata": {
        "id": "9a1pDmrvLym4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df.head()"
      ],
      "metadata": {
        "id": "CvB9ukRxJjnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 3.2 Working on \"SibSp\" & \"Parch\" columns (1 Mark)\n",
        "Combine columns \"SibSp\" & \"Parch\" and create another column that represents the total passengers in one ticket with the name \"family_size\". In each ticket, there might be Siblings/Spouses (SibSp =Number of Siblings/Spouses Aboard) or Parents/Children (Parch=Number of Parents/Children Aboard ) along with the passenger who booked the ticket.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "mUm20kyHVTZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create 'family_size' column, add Sibsp and Parch columns.\n",
        "X_train_df['family_size'] = X_train_df['SibSp'] + X_train_df['Parch']"
      ],
      "metadata": {
        "id": "rFMoAvDFCJgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mfm98C7mCbaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Working on the\"Embarked\" column (2 Marks)\n",
        "The \"embarked\" column represents the port of Embarkation: Cherbourg(C), Queenstown(Q), and  Southampton(S ). Thus, the entries are of three categories in this column. Fill in the missing rows in this column. We can fill it with the most frequent category. Map these categorical string entries into numerical.\n",
        "\n"
      ],
      "metadata": {
        "id": "cGPgnKttVYRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = X_train_df['Embarked'].value_counts()\n",
        "value_counts"
      ],
      "metadata": {
        "id": "IApw-rL1Y1jE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_string = value_counts.idxmax()\n",
        "max_string"
      ],
      "metadata": {
        "id": "rVTiJ27OVbzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['Embarked'].fillna(max_string, inplace=True)"
      ],
      "metadata": {
        "id": "0TFv47f9Y0yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['Embarked'].info()\n",
        "X_train_df.shape[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mi_8cdE-g-qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Working on the \"Age\" column (2 Marks)\n",
        "find the number of NaN entries in the age column and their row index. Calculate the mean, Standard deviation of the Age column and check the distribution of the age column.We can fill the missing values with randomly generated integer values between (mean+Standard deviation, mean-Standard deviation). Use : np.isnan; np.random.randint; concept of slicing dataframe. Convert the age column as an integer data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "dWmjUnN3VcF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#np.isnan; np.random.randint; concept of slicing dataframe. TBC\n",
        "Age_na_mask = X_train_df['Age'].isna()\n",
        "\n",
        "#count the number of NAN values\n",
        "nan_count =Age_na_mask.sum()\n",
        "\n",
        "#capture the indexes where the value is NAN\n",
        "nan_index_list = X_train_df.index[Age_na_mask].tolist()"
      ],
      "metadata": {
        "id": "gbLqDB1hVf1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NAN Count=\",nan_count)\n",
        "print(\"index list\", nan_index_list)"
      ],
      "metadata": {
        "id": "MFrug-3VakhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Calculate the mean of the column\n",
        "mean_value = X_train_df['Age'].mean()\n",
        "\n",
        "# Calculate the standard deviation of the column\n",
        "std_value = X_train_df['Age'].std()\n",
        "\n",
        "min_value = int(mean_value - std_value)\n",
        "max_value = int(mean_value + std_value)\n",
        "\n",
        "for index in nan_index_list:\n",
        "  X_train_df.at[index, 'Age'] = np.random.randint(min_value, max_value)\n",
        "\n",
        "#Convert column to int\n",
        "X_train_df['Age'] = X_train_df['Age'].astype(int)"
      ],
      "metadata": {
        "id": "a9a69SMpa6aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_value)\n",
        "print(std_value)\n",
        "print(min_value)\n",
        "print(max_value)\n",
        "#X_train_df['Age'].isnull().sum()\n",
        "X_train_df['Age'].head()"
      ],
      "metadata": {
        "id": "77m1Admwylc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMe9tkLAyrZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Working on \"sex\" column (1 Mark)\n",
        "Map the Sex column as 'female' : 0, 'male': 1, and convert it into an integer data type.\n",
        "\n"
      ],
      "metadata": {
        "id": "doeanDr0VgGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dictionary\n",
        "mapping_dict = {'female':0, 'male':1}\n",
        "\n",
        "#map string to integer\n",
        "X_train_df['Sex'] = X_train_df['Sex'].map(mapping_dict)\n",
        "\n",
        "#covert it to int type\n",
        "X_train_df['Sex'] = X_train_df['Sex'].astype(int)"
      ],
      "metadata": {
        "id": "dGN92EsEVlTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['Sex'].head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vFcZ5-971G5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6  Optional- Working on the \"Name\" column :\n",
        "Fetch titles from the name. We can map these titles with numbers and convert them into an integer. Use: concept of the regular expression.\n",
        "\n",
        "### 3.7 Optional- Working on the \"Fare\" column :\n",
        "We can convert face into categorical entries like Low, Medium, and High.\n",
        "\n"
      ],
      "metadata": {
        "id": "__CWnlGhVln2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find bins of that Fare column\n",
        "bins = pd.cut(dataset['Fare'], bins=3)\n",
        "# Get the boundaries of the bins\n",
        "bin_boundaries = bins.unique()\n",
        "\n",
        "print(bin_boundaries)\n",
        "\n",
        "dataset['Fare'].max()\n",
        "dataset['Fare'].min()\n",
        "dataset['Fare'].average()\n",
        "#required labels\n",
        "#Labels = ['Low', 'Medium', 'High']\n",
        "\n",
        "#dataset['Fare'] = pd.cut(dataset['Fare'], bins = bin_boundaries, labels = Labels, include_lowest=True)\n",
        "#dataset['Fare'].head()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vuuDE0mQVo7v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 Drop the columns (1 Mark)\n",
        "\n",
        "Drop the columns: - \"PassengerId\", \"Name\",  \"SibSp\" & \"Parch\", \"Tickets\", \"Cabin\"\n",
        "\n",
        "Now apply different ML algorithms and check the accuracy of your model.\n",
        "\n"
      ],
      "metadata": {
        "id": "6oJI0bQOVpP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_data = pd.DataFrame(X_train)\n",
        "#X_train_data"
      ],
      "metadata": {
        "id": "1ZlnDtiiVvif",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_train = X_train_df.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'],inplace = False)\n",
        "df_X_train.info()\n",
        "#df_X_train"
      ],
      "metadata": {
        "id": "YVYnt5p7ccKK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.9 Apply Standard Scalar (1 Mark)"
      ],
      "metadata": {
        "id": "S2EfoVojebWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "#for column in df_X_train.columns:\n",
        "  #df_X_train[column] = le.fit_transform(df_X_train[column])\n",
        "\n",
        "df_X_train['Embarked'] = le.fit_transform(df_X_train['Embarked'])\n",
        "\n",
        "df_X_train.head()\n",
        "#df_X_train.info()"
      ],
      "metadata": {
        "id": "GSaThxzakRG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_X_train)\n",
        "X_training = scaler.transform(df_X_train)\n",
        "X_training.shape, y_train.shape"
      ],
      "metadata": {
        "id": "1zo-rS-gge1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_training"
      ],
      "metadata": {
        "id": "7FPZqRydLm9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.10 Create a single function for preprocessing the test set (X_test) and apply it. (4 Marks)\n",
        "#### **Note**: All the pre-processing steps that were applied on the train set before ML Modelling are also applied on the test set before passing through the predict function."
      ],
      "metadata": {
        "id": "Kwa6Ua9Qgbi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Data_Pre_Processing(X_test):\n",
        "  ############## Create Has_Cabin Column ##############\n",
        "  for index in range(X_test.shape[0]):\n",
        "    ConvertStrToNum(X_test, 'Cabin')\n",
        "\n",
        "  ############## Create family_size Column ##############\n",
        "  X_test['family_size'] = X_test['SibSp'] + X_test['Parch']\n",
        "\n",
        "  ############## Process Embarked Column ##############\n",
        "  value_counts = X_test['Embarked'].value_counts()\n",
        "  max_string = value_counts.idxmax()\n",
        "  X_test['Embarked'].fillna(max_string, inplace=True)\n",
        "\n",
        "  ############## Work with Age Column ##############\n",
        "  Age_na_mask = X_test['Age'].isna()\n",
        "  #count the number of NAN values\n",
        "  nan_count =Age_na_mask.sum()\n",
        "  #capture the indexes where the value is NAN\n",
        "  nan_index_list = X_test.index[Age_na_mask].tolist()\n",
        "  # Calculate the mean of the column\n",
        "  mean_value = X_test['Age'].mean()\n",
        "  # Calculate the standard deviation of the column\n",
        "  std_value = X_test['Age'].std()\n",
        "\n",
        "  min_value = int(mean_value - std_value)\n",
        "  max_value = int(mean_value + std_value)\n",
        "\n",
        "  for index in nan_index_list:\n",
        "    X_test.at[index, 'Age'] = np.random.randint(min_value, max_value)\n",
        "\n",
        "  #Convert column to int\n",
        "  X_test['Age'] = X_test['Age'].astype(int)\n",
        "\n",
        "  ############## Create with Sex Column ##############\n",
        "  #create dictionary\n",
        "  mapping_dict = {'female':0, 'male':1}\n",
        "  #map string to integer\n",
        "  X_test['Sex'] = X_test['Sex'].map(mapping_dict)\n",
        "  #covert it to int type\n",
        "  X_test['Sex'] = X_test['Sex'].astype(int)\n",
        "\n",
        "  return X_test"
      ],
      "metadata": {
        "id": "0YR_Xioq64c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_col_list = X_test.columns.tolist()\n",
        "X_test_df = pd.DataFrame(X_test, columns=X_test_col_list)\n",
        "X_test_df = Data_Pre_Processing(X_test_df)\n",
        "X_test_df.info()"
      ],
      "metadata": {
        "id": "RLFNNM0SgqZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_df.head()"
      ],
      "metadata": {
        "id": "mtm0bHG5MxjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.11 Apply standard Scalar transformation to x_test (1 Mark)"
      ],
      "metadata": {
        "id": "jlA3Gnmrc039"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test_data = pd.DataFrame(X_test)\n",
        "\n",
        "df_X_test = X_test_df.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'],inplace = False)\n",
        "df_X_test.info()"
      ],
      "metadata": {
        "id": "N0SAb9ccc2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "#for column in df_X_test.columns:\n",
        "#    df_X_test[column] = le.fit_transform(df_X_test[column])\n",
        "df_X_test['Embarked'] = le.fit_transform(df_X_test['Embarked'])\n",
        "df_X_test.head()\n",
        "df_X_test.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "7d5aVsU-ao9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_X_test)\n",
        "df_X_test = scaler.transform(df_X_test)\n",
        "df_X_test.shape, y_test.shape\n",
        "#df_X_test.info()"
      ],
      "metadata": {
        "id": "CDn9tsLdbxq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gpTtB1fS4aRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  4. Apply Multiple ML Algo. along with  Ensemble Technique (Voting classifier) and display the accuracy (7 Marks)\n",
        "#### Expected Accuracy >= 80%  \n"
      ],
      "metadata": {
        "id": "zUMFQj-Gc1BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "Xd-21QgEc2TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv = SVC(kernel='linear')\n",
        "dt = DecisionTreeClassifier(max_depth=5)\n",
        "lg = LogisticRegression(max_iter=300)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "vc = VotingClassifier(estimators=[('sv',sv),('dt',dt),('lg',lg),('knn',knn)],voting='hard')"
      ],
      "metadata": {
        "id": "wn4F5_DUoX7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vc.fit(df_X_train, y_train)\n",
        "pred_final = vc.predict(df_X_train)\n",
        "print(accuracy_score(pred_final, y_train))"
      ],
      "metadata": {
        "id": "ZNg5RbcAmzQV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_X_train.head()\n",
        "y_train.head()\n"
      ],
      "metadata": {
        "id": "grExJE3dYiv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_final"
      ],
      "metadata": {
        "id": "hydhEn33OXOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  5. Pre-process the test_set (3 Marks)\n",
        "Again we have to apply the same preprocess function and standard scaler on this test set before passing through predict function.\n",
        "\n",
        "#### Understanding the test set:"
      ],
      "metadata": {
        "id": "NIf7BgedjLZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = pd.read_csv('/content/test_titanic.csv')"
      ],
      "metadata": {
        "id": "A4ApkkLec2V7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dataset_test.info())\n",
        "#print(dataset_test.head())\n",
        "dataset_test.isna().sum()"
      ],
      "metadata": {
        "id": "aBDpNj-5TdV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: In the initial train set there were no missing entries in the \"Fare\" column. But, now for the submission test set, there is one missing entry in this column.\n",
        "\n",
        "#### There will be a minor change in the preprocess function to address the above issue."
      ],
      "metadata": {
        "id": "syRBMp7ilrbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_df = Data_Pre_Processing(dataset_test)\n",
        "X_df.info()"
      ],
      "metadata": {
        "id": "Ppk5Fq0olrGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_df.isna().sum()"
      ],
      "metadata": {
        "id": "hQ4Lsp6znhrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_df['Fare'].fillna(X_df['Fare'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "jezCQu7WeiMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_df.isna().sum()"
      ],
      "metadata": {
        "id": "cLCW_39-fcFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise  6. Prediction for test data (2 Mark)"
      ],
      "metadata": {
        "id": "c-zATg3NnlKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_file = X_df.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'],inplace = False)\n",
        "dataset_test_file.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "bvDpq5EHnkcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_file.head()"
      ],
      "metadata": {
        "id": "IcX5i9ohf5Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_test_file = LabelEncoder()\n",
        "#for column in dataset_test_file.columns:\n",
        "#    dataset_test_file[column] = le_test_file.fit_transform(dataset_test_file[column])\n",
        "dataset_test_file['Embarked'] = le_test_file.fit_transform(dataset_test_file['Embarked'])\n",
        "\n",
        "dataset_test_file.info()"
      ],
      "metadata": {
        "id": "kJL2nq39f0JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_test_file = StandardScaler()\n",
        "#scaler_test_file.fit(dataset_test_file)\n",
        "#dataset_test_file_1 = scaler_test_file.transform(dataset_test_file)\n",
        "dataset_test_file_1 = scaler_test_file.fit_transform(dataset_test_file)\n",
        "dataset_test_file_1.shape\n",
        "\n",
        "col_list_final = dataset_test_file.columns.tolist()\n",
        "df_final = pd.DataFrame(dataset_test_file_1, columns=col_list_final)\n",
        "\n",
        "pred_test_file = vc.predict(df_final)\n",
        "#pred_test_file.shape\n",
        "#y_test.shape\n",
        "pred_test_file"
      ],
      "metadata": {
        "id": "-FnAHQCCgSu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "fRmjIGkLg_Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_final[:1]\n",
        "pred_test_file = vc.predict(df_final[:1])\n",
        "pred_test_file"
      ],
      "metadata": {
        "id": "j4GepUR_kRLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}